{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold = np.nan, linewidth = 115)\n",
    "import pickle\n",
    "\n",
    "# Load with pickle instead of processing images again\n",
    "training_img_1 = pickle.load(open('1vAll_img_res_Infiltration_1st_half.p', 'rb'))\n",
    "training_img_2 = pickle.load(open('1vAll_img_res_Infiltration_2nd_half.p', 'rb'))\n",
    "\n",
    "training_img_one = np.append(training_img_1, training_img_2, axis=0)\n",
    "\n",
    "training_img_3 = pickle.load(open('1vAll_img_res_Infiltration_3rd_half.p', 'rb'))\n",
    "training_img_4 = pickle.load(open('1vAll_img_res_Infiltration_4th_half.p', 'rb'))\n",
    "\n",
    "training_img_two = np.append(training_img_3, training_img_4, axis=0)\n",
    "\n",
    "training_img = np.append(training_img_one, training_img_two, axis=0)\n",
    "\n",
    "training_img.shape\n",
    "\n",
    "val_img = training_img[:2269]\n",
    "val_img = np.append(val_img, training_img[34029:], axis=0)\n",
    "training_img = training_img[2269:34029]\n",
    "print(len(val_img))\n",
    "print(len(training_img))\n",
    "print(len(training_img) + len(val_img))\n",
    "\n",
    "labels_1 = pickle.load(open('1vAll_labels_res_Infiltration_1st_half.p', 'rb'))\n",
    "labels_2 = pickle.load(open('1vAll_labels_res_Infiltration_2nd_half.p', 'rb'))\n",
    "labels_3 = pickle.load(open('1vAll_labels_res_Infiltration_3rd_half.p', 'rb'))\n",
    "labels_4 = pickle.load(open('1vAll_labels_res_Infiltration_4th_half.p', 'rb'))\n",
    "\n",
    "training_labels = np.append(labels_1, np.append(labels_2, np.append(labels_3, labels_4, axis = 0), axis = 0), axis = 0)\n",
    "\n",
    "val_labels = training_labels[:2269]\n",
    "val_labels = np.append(val_labels, training_labels[34029:], axis=0)\n",
    "training_labels = training_labels[2269:34029]\n",
    "print(len(val_labels))\n",
    "print(len(training_labels))\n",
    "print(len(training_labels) + len(val_labels))\n",
    "\n",
    "test_img = pickle.load(open('1vAll_test_img.p', 'rb'))\n",
    "test_labels = pickle.load(open('1vAll_test_labels.p', 'rb'))\n",
    "\n",
    "print('Labels shape: ', training_labels.shape)\n",
    "print('Length of test_labels: ', len(test_labels))\n",
    "print('No. of Infiltration Diagnoses: ', sum(training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models, optimizers, layers, regularizers, metrics, losses\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU, ELU, ReLU, ThresholdedReLU\n",
    "from keras.layers.core import Dense, Dropout, SpatialDropout2D, Activation\n",
    "from keras.layers.convolutional import Conv2D, SeparableConv2D\n",
    "from keras.models import model_from_json, Sequential\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config = config)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IMG_SIZE = 256\n",
    "\n",
    "# Save Comparison model\n",
    "def save_model(model_name, hist_str, model_str):\n",
    "\n",
    "    pickle.dump(model_name.history, open('Training Histories/'+ hist_str + '.p', 'wb'))\n",
    "    \n",
    "    print(\"Saved \" + hist_str + \" to Training Histories folder\")\n",
    "    \n",
    "    # serialize model to JSON\n",
    "    model_name = model.to_json()\n",
    "    with open(\"CNN Models/\" + model_str + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_name)\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"CNN Models/\" + model_str + \".h5\")\n",
    "    print(\"Saved \" + model_str + \" and weights to CNN Models folder\")\n",
    "    \n",
    "# Load model architecture and weights NOTE: must compile again\n",
    "def load_model():\n",
    "    model_str = str(input(\"Name of model to load: \"))\n",
    "\n",
    "    # load json and create model\n",
    "    json_file = open('CNN Models/' + model_str + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"CNN Models/\" + model_str + \".h5\")\n",
    "    print(\"Loaded \" + model_str + \" and weights from CNN Models folder\")\n",
    "    \n",
    "    return loaded_model\n",
    "    \n",
    "# Load history object\n",
    "def load_history():\n",
    "    hist_str = str(input(\"Name of history to load: \"))\n",
    "\n",
    "    loaded_history = pickle.load(open('Training Histories/' + hist_str + '.p', 'rb'))\n",
    "    \n",
    "    print(\"Loaded \" + hist_str + \" from Training Histories folder\")\n",
    "    \n",
    "    return loaded_history\n",
    "\n",
    "class True_Eval(Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        self.validation_data = validation_data\n",
    "        self.total_accuracy = []\n",
    "        self.i_accuracy = []\n",
    "        self.e_accuracy = []\n",
    "    \n",
    "    def ie_real_acc(self, prediction):\n",
    "        y_true = self.validation_data[1]\n",
    "        i_acc = 0\n",
    "        i_total = 0\n",
    "        e_acc = 0\n",
    "        e_total = 0\n",
    "        for i in range(0, len(prediction)):\n",
    "            if (y_true[i].round() == 0):\n",
    "                if (prediction[i].round() == y_true[i]):\n",
    "                    i_acc += 1\n",
    "                i_total += 1\n",
    "            else:\n",
    "                if (prediction[i].round() == y_true[i]):\n",
    "                    e_acc += 1\n",
    "                e_total += 1\n",
    "        return (i_acc/i_total), (e_acc/e_total)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x_val = self.validation_data[0]\n",
    "        y_pred = self.model.predict(x_val)\n",
    "        i_real_acc, e_real_acc = self.ie_real_acc(y_pred)\n",
    "        print (\"T Acc: %f\" % i_real_acc)\n",
    "        print (\"F Acc: %f\" % e_real_acc)\n",
    "        self.i_accuracy.append(i_real_acc)\n",
    "        self.e_accuracy.append(e_real_acc)\n",
    "        \n",
    "\n",
    "class MultiLabel_Acc(Callback):\n",
    "    \n",
    "    def __init__(self, validation_data):\n",
    "        self.validation_data = validation_data\n",
    "        self.accuracy = []\n",
    "        \n",
    "    def getAccuracy(self, prediction):\n",
    "        y_true = self.validation_data[1]\n",
    "        \n",
    "        correct = []\n",
    "        total = []\n",
    "        accuracy = []\n",
    "        \n",
    "        for i in range(0, len(y_true[0])):\n",
    "            correct.append([0, 0])\n",
    "            total.append([0, 0])\n",
    "        \n",
    "        for sample in range(0, len(prediction)):\n",
    "            for neuron in range(0, len(prediction[sample])):\n",
    "                \n",
    "                if (y_true[sample][neuron] == 0.0):\n",
    "                    if (round(prediction[sample][neuron]) == y_true[sample][neuron]):\n",
    "                        correct[neuron][0] += 1\n",
    "                    total[neuron][0] += 1\n",
    "                    \n",
    "                if (y_true[sample][neuron] == 1.0):\n",
    "                    if (round(prediction[sample][neuron]) == y_true[sample][neuron]):\n",
    "                        correct[neuron][1] += 1\n",
    "                    total[neuron][1] += 1\n",
    "        \n",
    "        for neuron in range(0, len(correct)):\n",
    "            accuracy.append((correct[neuron][0]/total[neuron][0],\n",
    "                                  correct[neuron][1]/total[neuron][1]))\n",
    "        return accuracy\n",
    "                    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.validation_data[0])\n",
    "        epoch_data = self.getAccuracy(y_pred)\n",
    "        self.accuracy.append(epoch_data)\n",
    "        \n",
    "        for i in range(0, len(epoch_data)):\n",
    "            print(\"         Neuron: #\"+ str(i + 1))\n",
    "            print(\"Zeroes Accuracy:\", epoch_data[i][0])\n",
    "            print(\"  Ones Accuracy:\", epoch_data[i][1])\n",
    "            \n",
    "            \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                         normalize = False,\n",
    "                         title = 'Confusion Matrix',\n",
    "                         cmap=plt.cm.Blues):\n",
    "    _fontsize = 'xx-large'\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "        print(\"Normalized Confusion Matrix\")\n",
    "    else:\n",
    "        print(\"Confusion Matrix without Normalization\")\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=_fontsize)\n",
    "    cb = plt.colorbar()\n",
    "    cb.ax.tick_params(labelsize=15)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation = 45, fontsize=_fontsize) \n",
    "    plt.yticks(tick_marks, classes, fontsize=_fontsize)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.min() + 0.2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                horizontalalignment = 'center',\n",
    "                color='white' if cm[i, j] > thresh else 'black', \n",
    "                fontsize=_fontsize)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Labels', fontsize=_fontsize)\n",
    "    plt.xlabel('Predicted Labels', fontsize=_fontsize)\n",
    "\n",
    "# Metric Analysis\n",
    "def _1vAll_accuracy(y_test, pred):\n",
    "    \n",
    "    pred = np.squeeze(pred, axis = -1)\n",
    "    pred = np.round_(pred)\n",
    "    pred = pred.astype(dtype = 'uint8')\n",
    "    \n",
    "    ft = pred == y_test\n",
    "    \n",
    "    accuracy = sum(ft)/len(ft)\n",
    "        \n",
    "    print('\\t Complete Label Accuracy: %.2f' % round((accuracy * 100), 2), '%')\n",
    "    \n",
    "    print('Sum of Fully Correct Predictions: ', sum(ft))\n",
    "    print('\\t\\t    Total Labels: ', len(ft))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Deep Residual Neural Network - XVNet for Paperspace\n",
    "\n",
    "'''     \n",
    "\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "img_channels = 1\n",
    "\n",
    "#\n",
    "# network params\n",
    "#\n",
    "\n",
    "cardinality = 16\n",
    "\n",
    "\n",
    "def residual_network(x):\n",
    "    \"\"\"\n",
    "    ResNeXt by default. For ResNet set `cardinality` = 1 above.\n",
    "    \n",
    "    \"\"\"\n",
    "    def add_common_layers(y):\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        y = layers.ReLU()(y)\n",
    "        #y = layers.SpatialDropout2D(0.125)(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def grouped_convolution(y, nb_channels, _strides):\n",
    "        # when `cardinality` == 1 this is just a standard convolution\n",
    "        if cardinality == 1:\n",
    "            return layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "        \n",
    "        assert not nb_channels % cardinality\n",
    "        _d = nb_channels // cardinality\n",
    "\n",
    "        # in a grouped convolution layer, input and output channels are divided into `cardinality` groups,\n",
    "        # and convolutions are separately performed within each group\n",
    "        groups = []\n",
    "        for j in range(cardinality):\n",
    "            group = layers.Lambda(lambda z: z[:, :, :, j * _d:j * _d + _d])(y)\n",
    "            groups.append(layers.Conv2D(_d, kernel_size=(3, 3), strides=_strides, padding='same')(group))\n",
    "            \n",
    "        # the grouped convolutional layer concatenates them as the outputs of the layer\n",
    "        y = layers.concatenate(groups)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def residual_block(y, nb_channels_in, nb_channels_out, _strides=(1, 1), _project_shortcut=False):\n",
    "        \"\"\"\n",
    "        Our network consists of a stack of residual blocks. These blocks have the same topology,\n",
    "        and are subject to two simple rules:\n",
    "        - If producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes).\n",
    "        - Each time the spatial map is down-sampled by a factor of 2, the width of the blocks is multiplied by a factor of 2.\n",
    "        \"\"\"\n",
    "        shortcut = y\n",
    "\n",
    "        # we modify the residual building block as a bottleneck design to make the network more economical\n",
    "        y = layers.Conv2D(nb_channels_in, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
    "        y = add_common_layers(y)\n",
    "\n",
    "        # ResNeXt (identical to ResNet when `cardinality` == 1)\n",
    "        y = grouped_convolution(y, nb_channels_in, _strides=_strides)\n",
    "        y = add_common_layers(y)\n",
    "\n",
    "        y = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
    "        # batch normalization is employed after aggregating the transformations and before adding to the shortcut\n",
    "        y = layers.BatchNormalization()(y)\n",
    "\n",
    "        # identity shortcuts used directly when the input and output are of the same dimensions\n",
    "        if _project_shortcut or _strides != (1, 1):\n",
    "            # when the dimensions increase projection shortcut is used to match dimensions (done by 1Ã—1 convolutions)\n",
    "            # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
    "            shortcut = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
    "            shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "        y = layers.add([shortcut, y])\n",
    "\n",
    "        # relu is performed right after each batch normalization,\n",
    "        # expect for the output of the block where relu is performed after the adding to the shortcut\n",
    "        y = layers.ReLU()(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    # conv block 1\n",
    "    x = layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = add_common_layers(x)\n",
    "    x = layers.MaxPool2D(pool_size=(2, 2), strides=None, padding='same')(x)\n",
    "    \n",
    "    # residual block\n",
    "    for i in range(3):\n",
    "        project_shortcut = (i == 0)\n",
    "        x = residual_block(x, 16, 32, _project_shortcut=project_shortcut)\n",
    "\n",
    "    # conv block 2\n",
    "    x = layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = add_common_layers(x)    \n",
    "    x = layers.MaxPool2D(pool_size=(2, 2), strides=None, padding='same')(x)\n",
    "    \n",
    "    # conv block 3\n",
    "    x = layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = add_common_layers(x)    \n",
    "    x = layers.MaxPool2D(pool_size=(2, 2), strides=None, padding='same')(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(4):\n",
    "        # down-sampling is performed by conv3_1, conv4_1, and conv5_1 with a stride of 2\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 32, 64, _strides=strides)\n",
    "\n",
    "    # conv4\n",
    "    for i in range(6):\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 64, 128, _strides=strides)\n",
    "\n",
    "    # conv5\n",
    "    for i in range(3):\n",
    "        strides = (2, 2) if i == 0 else (1, 1)\n",
    "        x = residual_block(x, 128, 256, _strides=strides)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    #x = layers.Dense(16)(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n",
    "network_output = residual_network(image_tensor)\n",
    "  \n",
    "model = models.Model(inputs=[image_tensor], outputs=[network_output])\n",
    "print(model.summary())\n",
    "\n",
    "# Last model: XVNet_250e\n",
    "#model = load_model()\n",
    "#model.summary()\n",
    "\n",
    "model.compile(optimizer = optimizers.RMSprop(lr = 1e-4), \n",
    "              loss = 'binary_crossentropy', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "custom_metrics = MultiLabel_Acc((val_img, val_labels))\n",
    "\n",
    "model_obj = model.fit(training_img, training_labels, \n",
    "                      epochs = 250, initial_epoch = 0, \n",
    "                      validation_data = (val_img, val_labels), \n",
    "                      batch_size = 128, verbose = 2, \n",
    "                      callbacks = [custom_metrics])\n",
    "\n",
    "Predictions = model.predict(test_img)\n",
    "\n",
    "Accuracy = _1vAll_accuracy(test_labels, Predictions)\n",
    "\n",
    "history_str = 'XVNet_250e_history'\n",
    "model_str   = 'XVNet_250e'\n",
    "    \n",
    "save_model(model_obj, history_str, model_str)\n",
    "\n",
    "acc = model_obj.history['acc']\n",
    "val_acc = model_obj.history['val_acc']\n",
    "loss = model_obj.history['loss']\n",
    "val_loss = model_obj.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(epochs, acc, 'bd', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy', fontsize='xx-large')\n",
    "plt.xticks(fontsize='xx-large')\n",
    "plt.xlabel('Epochs', fontsize='xx-large')\n",
    "plt.yticks(fontsize='xx-large')\n",
    "plt.ylabel('Accuracy', fontsize='xx-large')\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.show()\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(epochs, loss, 'rd', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss', fontsize='xx-large')\n",
    "plt.xticks(fontsize='xx-large')\n",
    "plt.xlabel('Epochs', fontsize='xx-large')\n",
    "plt.yticks(fontsize='xx-large')\n",
    "plt.ylabel('Loss', fontsize='xx-large')\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.show()\n",
    "\n",
    "cust_acc = custom_metrics.accuracy\n",
    "epochs = range(1, len(cust_acc) + 1)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(epochs, cust_acc[epochs][0][0], 'g', label='True Accuracy')\n",
    "plt.title('True Accuracy', fontsize='xx-large')\n",
    "plt.xticks(fontsize='xx-large')\n",
    "plt.xlabel('Epochs', fontsize='xx-large')\n",
    "plt.yticks(fontsize='xx-large')\n",
    "plt.ylabel('Accuracy', fontsize='xx-large')\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.show()\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "cm = confusion_matrix(test_labels, np.round_(Predictions))\n",
    "cm_plot_labels = ['Not Infiltration', 'Infiltration']\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize = (20, 10))\n",
    "plot_confusion_matrix(cm, cm_plot_labels, title = model_str, normalize=True)\n",
    "#save_plt(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
